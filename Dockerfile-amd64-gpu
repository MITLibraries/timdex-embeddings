# NOTE: 763104351884.dkr.ecr.us-east-1.amazonaws.com is preferred over public.ecr.aws/deep-learning-containers
# when building in AWS to minimize network traffic and latency.  To install locall, you may need to authenticate
# for ECR image pulls with,
# 	aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com
FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-ec2

RUN apt-get update && \
    apt-get install -y --no-install-recommends git ca-certificates && \
    rm -rf /var/lib/apt/lists/*

COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv
ENV UV_SYSTEM_PYTHON=1

WORKDIR /app

# Copy project metadata
COPY pyproject.toml ./

# Install package into system python
RUN uv pip install --group dlc_amd64_gpu --system .

# Copy CLI application
COPY embeddings ./embeddings

# Copy fixtures
COPY tests/fixtures /fixtures

# Set environment variables
# NOTE: The env vars "TE_MODEL_URI" and "TE_MODEL_PATH" are set here to support
# the downloading of the model during image build, but also persist in the container
# and serve to set the default model.
ENV PYTHONPATH=/app
ENV HF_HUB_DISABLE_PROGRESS_BARS=true
ENV TE_MODEL_URI=opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte
ENV TE_MODEL_PATH=/model
ENV TE_TORCH_DEVICE=cuda

# Download the model and include in the Docker image
RUN embeddings --verbose download-model

ENTRYPOINT ["embeddings"]
