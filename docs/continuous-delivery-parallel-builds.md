# The CD pipeline for multiple, parallel builds

This application has a unique CD pipeline for pushing container images to AWS. All of our other applications will push either an AMD64-based image or an ARM64-based image to the ECR Repository while this one expects to build both.

This application is destined to run in a Compute Environment in AWS Batch and, depending on the data to be processed, the application might run better in an environment that has a GPU or it might run better in an environment without a GPU. (By "better" we mean both faster and at a lower cost. See [Addendum: Use AWS Batch as Compute for Embeddings](https://mitlibraries.atlassian.net/wiki/spaces/D/pages/4832493614/Engineering+Plan+Record+Embeddings+ETL#Addendum%3A-Use-AWS-Batch-as-Compute-for-Embeddings) for more details.)

## Two Dockerfiles

We separate the two builds by leveraging two different Dockerfiles:

* `Dockerfile-gpu`: The Dockerfile that defines the build for any GPU-enabled containers
* `Dockerfile-cpu`: The Dockerfile that defines the build for containers that will not use GPUs

At this time, AWS Batch only supports the `amd64` architecture for AWS Batch compute environments that leverage GPUs. Otherwise, for runs of this application that do not require GPUs, the `arm64` architecture is less expensive and more efficient.

## CPU Architecture

We will stick with a single `.aws-architecture` file to manage the CPU architecture choice, but we will format it as a simple list of key/value pairs and leverage `jq` to parse the information in our `make` commands and GitHub workflows. The file will look like

```json
{
    "gpu": "linux/<arch>",
    "cpu": "linux/<arch>"
}
```

Where `<arch>` is either `amd64` or `arm64`.

## Makefile configuration

There are a collection of targets in the Makefile for generating Docker images locally and pushing those local builds to the ECR Repository in the Dev1 AWS Account. The tags generated by running these targets will include the work `make` so that it is clear in the AWS Console that the image came from a developer, not from GitHub Actions.

## GitHub Actions

There are three GitHub Actions workflows for automated build+deploy to AWS. These do **NOT** depend on the `Makefile` targets at all. While the triggers follow our typical GitHub-flow, the actual build process is different from the rest of our application respositories

### Dev Workflow

1. There is an initial job that runs and parses the `.aws-architecture` file and generates outputs that will drive the next phase.
1. The second phase of the workflow is a matrix strategy that will kick off two runners, one for each build. The runner is picked to match the CPU architecture of the requested build. That is, if the `gpu` key in the `.aws-architecture` file specifies `linux/amd64`, then the runner for the `gpu` container will be an `amd64`-based runner. If the `cpu` key in the `.aws-architecture` file specifies `linux/arm64` then the `cpu` container will be an `arm64`-based runner. This way, when Docker runs, it is running on the same architecture as the container it is trying to build.

### Stage Workflow

The Stage workflow is the same as the Dev workflow, only the trigger is different (it runs on `push` instead of `pull_request`).

### Prod Workflow

Similar to our shared workflows, the Prod workflow will 

1. Verify that the SHAs match between stage & prod
1. Download the images from the Stage ECR
1. Re-tag the images for Prod
1. Upload the images to Prod ECR

There is no need for a matrix or different runners since we aren't building anything.
